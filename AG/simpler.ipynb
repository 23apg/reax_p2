{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\APG\\Anaconda3\\envs\\AngelosAwesomeEnvironment\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\APG\\Anaconda3\\envs\\AngelosAwesomeEnvironment\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\APG\\Anaconda3\\envs\\AngelosAwesomeEnvironment\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\APG\\Anaconda3\\envs\\AngelosAwesomeEnvironment\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from time import sleep\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tweepy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit():\n",
    "\n",
    "    reddit = praw.Reddit(client_id='OxteSNrREDwJUw', \\\n",
    "                     client_secret='FeY9Yx1ouGLTTpZRRNWXdpGWTIM', \\\n",
    "                     user_agent='nw_project2_getcities', \\\n",
    "                     username='hawkcatdogbird', \\\n",
    "                     password='23Reddog!')\n",
    "    return reddit       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_me(comment):\n",
    "    #feed me text strings, I dance with spacy\n",
    "    nlp_key = []\n",
    "    nlp_value = []\n",
    "    doc = nlp(comment)\n",
    "    if not doc.ents:\n",
    "        nlp_key.append(\"no key\")\n",
    "        nlp_value.append(\"no entry\")\n",
    "    else:\n",
    "        for ent in doc.ents:\n",
    "            if ent.text not in nlp_key:\n",
    "                nlp_key.append(ent.text)\n",
    "            elif ent.text not in nlp_value:\n",
    "                nlp_value.append(ent.label_)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return nlp_key, nlp_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas','California','Colorado', 'Connecticut','Delaware', 'Florida', \n",
    "'Georgia','Hawaii', 'Idaho', 'Illinois','Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', \n",
    "'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana','Nebraska', 'Nevada', 'NewHampshire', \n",
    "'NewJersey', 'NewMexico', 'NewYork', 'NorthCarolina', 'NorthDakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', \n",
    "'RhodeIsland', 'SouthCarolina', 'SouthDakota','Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', \n",
    "'WestVirginia', 'Wisconsin', 'Wyoming', 'washingtondc']\n",
    "\n",
    "city_subreddits = ['NYC', 'Seattle', 'Chicago', 'LosAngeles', 'Portland', 'Boston', 'Detroit',\n",
    "'SanFrancisco', 'Atlanta', 'Philadelphia', \"Columbus\", 'Dallas', 'Baltimore', 'StLouis', 'SanDiego', \n",
    "'Houston', \"Phoenix\", \"Orlando\", \"Miami\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reddit()\n",
    "\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "db = client.project_reax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_state_sub(subredditname, period):\n",
    "\n",
    "    words = ''\n",
    "    headlines = []\n",
    "    #pos\n",
    "    posheadlines = []\n",
    "    poskarma = []\n",
    "    commentspos = []\n",
    "    #neg\n",
    "    negheadlines = []\n",
    "    negkarma = []\n",
    "    commentsneg = []\n",
    "    #neu\n",
    "    neuheadlines = []\n",
    "    neukarma = []\n",
    "    commentsneu = []\n",
    "\n",
    "    headlinesentiment = []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    for submission in r.subreddit(subredditname).top(period):\n",
    "\n",
    "\n",
    "        headlines.append(submission.title)\n",
    "\n",
    "        words = words + \" \" + submission.title\n",
    "\n",
    "        results = analyzer.polarity_scores(submission.title)\n",
    "\n",
    "        cresults = results['compound']\n",
    "        headlinesentiment.append(cresults)\n",
    "\n",
    "        if (cresults > .2):\n",
    "            posheadlines.append(cresults)\n",
    "            poskarma.append(submission.score)\n",
    "            commentspos.append(submission.num_comments)\n",
    "\n",
    "        elif (cresults < (-0.2)):\n",
    "            negheadlines.append(cresults)\n",
    "            negkarma.append(submission.score)\n",
    "            commentsneg.append(submission.num_comments)\n",
    "        else:\n",
    "            neuheadlines.append(cresults)\n",
    "            neukarma.append(submission.score)\n",
    "            commentsneu.append(submission.num_comments)\n",
    "\n",
    "    headtot = len(headlines)\n",
    "    headp = len(posheadlines)\n",
    "    headneg = len(negheadlines)\n",
    "    headneu = len(neuheadlines)\n",
    "\n",
    "    percposhead = headp/headtot\n",
    "    percneghead = headneg/headtot\n",
    "    percneuhead = headneu/headtot\n",
    "    \n",
    "    posavgkarma = (np.sum(poskarma)/headp)\n",
    "    negavgkarma = (np.sum(negkarma)/headneg)\n",
    "    neuavgkarma = (np.sum(neukarma)/headneu)\n",
    "    \n",
    "    posavgcomms = (np.sum(commentspos)/headp)\n",
    "    negavgcomms = (np.sum(commentsneg)/headneg)\n",
    "    neuavgcomms = (np.sum(commentsneu)/headneu)\n",
    "\n",
    "    cs = np.mean(headlinesentiment)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "    text_tokens = []\n",
    "    toks = tokenizer.tokenize(words)\n",
    "    toks = [t.lower() for t in toks if t.lower() not in stop_words]\n",
    "    text_tokens.extend(toks)\n",
    "\n",
    "    text_frequency = nltk.FreqDist(text_tokens)\n",
    "\n",
    "    t_freq = text_frequency.most_common(30)\n",
    "\n",
    "    key_holder, value_holder = nlp_me(words)\n",
    "#     print(key_holder)\n",
    "       \n",
    "    \n",
    "    statedict = dict(location = subredditname,\n",
    "                     headlines = headlines,\n",
    "                     headlineCOMSENTAVG = cs,\n",
    "                     percposhead = percposhead,\n",
    "                     percneghead = percneghead,\n",
    "                     percneuhead = percneuhead,\n",
    "                     avgupscorePOSHL = posavgkarma,\n",
    "                     avgupscoreNEGHL = negavgkarma,\n",
    "                     avgupscoreNEUHL = neuavgkarma,\n",
    "                     posavgcomms = posavgcomms,\n",
    "                     negavgcomms = negavgcomms,\n",
    "                     neuavgcomms = neuavgcomms,                   \n",
    "                     POSheadlines = posheadlines,\n",
    "                     NEGheadlines = negheadlines,\n",
    "                     NEUheadlines = neuheadlines,\n",
    "                     wordcloud = t_freq,\n",
    "                     keys = key_holder)\n",
    "\n",
    "    db.state_simple.insert_one(statedict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_city_sub(subredditname, period):\n",
    "\n",
    "    words = ''\n",
    "    headlines = []\n",
    "    #pos\n",
    "    posheadlines = []\n",
    "    poskarma = []\n",
    "    commentspos = []\n",
    "    #neg\n",
    "    negheadlines = []\n",
    "    negkarma = []\n",
    "    commentsneg = []\n",
    "    #neu\n",
    "    neuheadlines = []\n",
    "    neukarma = []\n",
    "    commentsneu = []\n",
    "\n",
    "    headlinesentiment = []\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    for submission in r.subreddit(subredditname).top(period):\n",
    "\n",
    "\n",
    "        headlines.append(submission.title)\n",
    "\n",
    "        words = words + \" \" + submission.title\n",
    "\n",
    "        results = analyzer.polarity_scores(submission.title)\n",
    "\n",
    "        cresults = results['compound']\n",
    "        headlinesentiment.append(cresults)\n",
    "\n",
    "        if (cresults > .2):\n",
    "            posheadlines.append(cresults)\n",
    "            poskarma.append(submission.score)\n",
    "            commentspos.append(submission.num_comments)\n",
    "\n",
    "        elif (cresults < (-0.2)):\n",
    "            negheadlines.append(cresults)\n",
    "            negkarma.append(submission.score)\n",
    "            commentsneg.append(submission.num_comments)\n",
    "        else:\n",
    "            neuheadlines.append(cresults)\n",
    "            neukarma.append(submission.score)\n",
    "            commentsneu.append(submission.num_comments)\n",
    "\n",
    "    headtot = len(headlines)\n",
    "    headp = len(posheadlines)\n",
    "    headneg = len(negheadlines)\n",
    "    headneu = len(neuheadlines)\n",
    "\n",
    "    percposhead = headp/headtot\n",
    "    percneghead = headneg/headtot\n",
    "    percneuhead = headneu/headtot\n",
    "    \n",
    "    posavgkarma = (np.sum(poskarma)/headp)\n",
    "    negavgkarma = (np.sum(negkarma)/headneg)\n",
    "    neuavgkarma = (np.sum(neukarma)/headneu)\n",
    "    \n",
    "    posavgcomms = (np.sum(commentspos)/headp)\n",
    "    negavgcomms = (np.sum(commentsneg)/headneg)\n",
    "    neuavgcomms = (np.sum(commentsneu)/headneu)\n",
    "\n",
    "    cs = np.mean(headlinesentiment)\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "    text_tokens = []\n",
    "    toks = tokenizer.tokenize(words)\n",
    "    toks = [t.lower() for t in toks if t.lower() not in stop_words]\n",
    "    text_tokens.extend(toks)\n",
    "\n",
    "    text_frequency = nltk.FreqDist(text_tokens)\n",
    "\n",
    "    t_freq = text_frequency.most_common(30)\n",
    "\n",
    "    key_holder, value_holder = nlp_me(words)\n",
    "#     print(key_holder)\n",
    "    \n",
    "    if (subredditname == 'NYC'):\n",
    "        subredditname = 'New York'\n",
    "    else:\n",
    "        subredditname = subredditname\n",
    "        \n",
    "    date = dt.datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    statedict = dict(location = subredditname,\n",
    "                     headlines = headlines,\n",
    "                     headlineCOMSENTAVG = cs,\n",
    "                     percposhead = percposhead,\n",
    "                     percneghead = percneghead,\n",
    "                     percneuhead = percneuhead,\n",
    "                     avgupscorePOSHL = posavgkarma,\n",
    "                     avgupscoreNEGHL = negavgkarma,\n",
    "                     avgupscoreNEUHL = neuavgkarma,\n",
    "                     posavgcomms = posavgcomms,\n",
    "                     negavgcomms = negavgcomms,\n",
    "                     neuavgcomms = neuavgcomms,                   \n",
    "                     POSheadlines = posheadlines,\n",
    "                     NEGheadlines = negheadlines,\n",
    "                     NEUheadlines = neuheadlines,\n",
    "                     wordcloud = t_freq,\n",
    "                     keys = key_holder,\n",
    "                     date = data)\n",
    "\n",
    "    db.city_simple.insert_one(statedict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 'month'\n",
    "\n",
    "for state in states:\n",
    "\n",
    "    scrape_state_sub(state, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 'day'\n",
    "\n",
    "for city in city_subreddits:\n",
    "    \n",
    "    scrape_city_sub(city, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
